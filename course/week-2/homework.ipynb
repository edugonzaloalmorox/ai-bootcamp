{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "40a15a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import re\n",
    "from typing import Callable, Dict, Iterable, List, Optional, Tuple, Union, Set\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "from urllib.parse import urlparse, parse_qs, urlencode, urlunparse, unquote,quote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbbb231",
   "metadata": {},
   "source": [
    "# PART 1: PROJECT\n",
    "\n",
    "## Question 1: Data Ingestion \n",
    "\n",
    "The project involves extracting information from public bids, initially sourced from HTML and PDF formats. The HTML files contain bid and contract details, covering fields such as contract duration, contract type, and contract value. Meanwhile, the PDF documents specify the technical and legal requirements essential for securing the contract. The context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1b90d274",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "HEADERS = {\"User-Agent\": \"html-fetcher/1.0\"}\n",
    "BASE_URL = \"https://contratos-publicos.comunidad.madrid\"\n",
    "\n",
    "\n",
    "def fetch_html(url: str, timeout: int = 30) -> str:\n",
    "    \"\"\"Fetch raw HTML from a URL using requests.\"\"\"\n",
    "    r = requests.get(url, headers=HEADERS, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    r.encoding = r.apparent_encoding or \"utf-8\"\n",
    "    return r.text\n",
    "\n",
    "\n",
    "def extract_contract_links(html: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract links starting with /contrato-publico/.\n",
    "    Normalize to absolute URLs. Return unique list.\n",
    "    \"\"\"\n",
    "    pattern = r'href=\"(/contrato-publico/[^\"]+)\"'\n",
    "    matches = re.findall(pattern, html)\n",
    "    return sorted({BASE_URL + path for path in matches})\n",
    "\n",
    "\n",
    "def update_page(url: str, page: int) -> str:\n",
    "    \"\"\"Replace the `page` parameter in the URL with a new value.\"\"\"\n",
    "    parts = urlparse(url)\n",
    "    q = parse_qs(parts.query, keep_blank_values=True)\n",
    "    q[\"page\"] = [str(page)]\n",
    "    new_query = urlencode(q, doseq=True)\n",
    "    return urlunparse(parts._replace(query=new_query))\n",
    "\n",
    "\n",
    "def paginate_contract_links(\n",
    "    base_url: str,\n",
    "    max_pages: int = 20,\n",
    "    sleep_secs: float = 0.8,\n",
    "    stop_when_empty: bool = True,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Crawl pages ?page=0..N and collect all contract links.\n",
    "    \"\"\"\n",
    "    seen: Set[str] = set()\n",
    "\n",
    "    for page in range(max_pages):\n",
    "        url = update_page(base_url, page)\n",
    "        html = fetch_html(url)\n",
    "        links = extract_contract_links(html)\n",
    "\n",
    "        before = len(seen)\n",
    "        seen.update(links)\n",
    "        added = len(seen) - before\n",
    "        print(f\"[page {page}] found={len(links)} added={added} total={len(seen)}\")\n",
    "\n",
    "        if stop_when_empty and len(links) == 0:\n",
    "            print(\"No links found → stopping early.\")\n",
    "            break\n",
    "\n",
    "        time.sleep(sleep_secs)\n",
    "\n",
    "    return sorted(seen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3d488704",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "url = 'https://contratos-publicos.comunidad.madrid/contratos?t=&tipo_publicacion=All&createddate=&createddate_1=&fin_presentacion=&fin_presentacion_1=&ss_buscador_estado_situacion=All&numero_expediente=&referencia=&ss_identificador_ted=&entidad_adjudicadora=All&tipo_contrato=All&codigo_cpv=&ss_field_contrato_lote_reservado=All&bs_regulacion_armonizada=All&ss_sist_de_contratacion=All&modalidad_compra_publica=All&ss_financiacion_ue=All&ss_field_pcon_codigo_referencia=&procedimiento_adjudicacion=All&ss_tipo_de_tramitacion=All&ss_metodo_presentacion=All&bs_subasta_electronica=All&presupuesto_base_licitacion_total=&presupuesto_base_licitacion_total_1=&ds_field_pcon_fecha_desierto=&ds_field_pcon_fecha_desierto_1=&nif_adjudicatario=&nombre_adjudicatario=&importacion_adjudicacion_con_impuestos=&importacion_adjudicacion_con_impuestos_1=&ds_fecha_encargo=&ds_fecha_encargo_1=&ds_field_pcon_fecha_publi_anun_form=&ds_field_pcon_fecha_publi_anun_form_1=&bs_field_pcon_compra_publica=All&page=0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "00da0f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[page 0] found=10 added=10 total=10\n",
      "[page 1] found=10 added=10 total=20\n",
      "[page 2] found=10 added=10 total=30\n",
      "[page 3] found=10 added=10 total=40\n",
      "[page 4] found=10 added=10 total=50\n",
      "[page 5] found=10 added=10 total=60\n",
      "[page 6] found=10 added=10 total=70\n",
      "[page 7] found=10 added=10 total=80\n",
      "[page 8] found=10 added=10 total=90\n",
      "[page 9] found=10 added=10 total=100\n",
      "\n",
      "Collected 100 unique contract links ✅\n",
      "https://contratos-publicos.comunidad.madrid/contrato-publico/2025-0-78-suministro-instalacion-puesta-funcionamiento-equipamiento-laboratorio\n",
      "https://contratos-publicos.comunidad.madrid/contrato-publico/2025-0-80-suministro-instalacion-puesta-funcionamiento-cunas-termicas-fototerapia\n",
      "https://contratos-publicos.comunidad.madrid/contrato-publico/acuerdo-marco-servicio-asistencia-tecnica-materia-seguridad-salud-proyectos-0\n",
      "https://contratos-publicos.comunidad.madrid/contrato-publico/acuerdo-marco-servicios-mantenimiento-correctivo-segundo-nivel-equipos-0\n",
      "https://contratos-publicos.comunidad.madrid/contrato-publico/acuerdo-marco-trabajos-inspeccion-revision-control-desamiantado\n"
     ]
    }
   ],
   "source": [
    "# Get the links\n",
    "search_url = \"https://contratos-publicos.comunidad.madrid/contratos?t=&tipo_publicacion=All&createddate=&createddate_1=&fin_presentacion=&fin_presentacion_1=&ss_buscador_estado_situacion=All&numero_expediente=&referencia=&ss_identificador_ted=&entidad_adjudicadora=All&tipo_contrato=All&codigo_cpv=&ss_field_contrato_lote_reservado=All&bs_regulacion_armonizada=All&ss_sist_de_contratacion=All&modalidad_compra_publica=All&ss_financiacion_ue=All&ss_field_pcon_codigo_referencia=&procedimiento_adjudicacion=All&ss_tipo_de_tramitacion=All&ss_metodo_presentacion=All&bs_subasta_electronica=All&presupuesto_base_licitacion_total=&presupuesto_base_licitacion_total_1=&ds_field_pcon_fecha_desierto=&ds_field_pcon_fecha_desierto_1=&nif_adjudicatario=&nombre_adjudicatario=&importacion_adjudicacion_con_impuestos=&importacion_adjudicacion_con_impuestos_1=&ds_fecha_encargo=&ds_fecha_encargo_1=&ds_field_pcon_fecha_publi_anun_form=&ds_field_pcon_fecha_publi_anun_form_1=&bs_field_pcon_compra_publica=All&page=0\"\n",
    "\n",
    "contract_urls = paginate_contract_links(search_url, max_pages=10)\n",
    "print(f\"\\nCollected {len(contract_urls)} unique contract links ✅\")\n",
    "\n",
    "for link in contract_urls[:5]:\n",
    "    print(link)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f83e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_convocatoria_from_url(url: str, timeout: int = 30) -> Dict[str, Optional[str]]:\n",
    "    \"\"\"\n",
    "    Downloads a contract detail page, parses <ul class='pcon-convocatoria'>,\n",
    "    and returns a dictionary {label: value}.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        resp = requests.get(url, headers=HEADERS, timeout=timeout)\n",
    "        resp.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"Request error for {url}: {e}\")\n",
    "        return {}\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    data: Dict[str, Optional[str]] = {}\n",
    "\n",
    "    ul = soup.select_one(\"ul.pcon-convocatoria\")\n",
    "    if not ul:\n",
    "        return data\n",
    "\n",
    "    for li in ul.find_all(\"li\", recursive=False):\n",
    "        label_el = li.select_one(\".field__label\")\n",
    "        if not label_el:\n",
    "            continue\n",
    "\n",
    "        label = label_el.get_text(strip=True)\n",
    "\n",
    "        # Prefer .field__item but fallback to .field-content\n",
    "        value_el = li.select_one(\".field__item\") or li.select_one(\".field-content\")\n",
    "\n",
    "        if value_el:\n",
    "            value = \" \".join(value_el.stripped_strings)\n",
    "        else:\n",
    "            value = None\n",
    "\n",
    "        data[label] = value\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b08689c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readings: https://contratos-publicos.comunidad.madrid/contrato-publico/2025-0-78-suministro-instalacion-puesta-funcionamiento-equipamiento-laboratorio .... \n",
      "\n",
      "{'Tipo de publicación': 'Convocatoria anunciada a licitación', 'Situación': 'En plazo', 'Número de expediente': '2025-0-78', 'Referencia': 'C11145', 'Objeto del contrato': 'Suministro, instalación y puesta en funcionamiento de equipamiento de laboratorio para la unidad de reproducción humana del Hospital Universitario 12 de Octubre.', 'Tipo de contrato': 'Suministros', 'Contrato mixto': 'No', 'Código CPV': '33100000-1', 'Contrato/lote reservado': 'No aplica', 'Legislación nacional aplicable': 'Ley 9/2017', 'Sujeto a regulación armonizada': 'No', 'Sistema de contratación': 'No aplica', 'Código NUTS': 'ES300', 'Compra pública de innovación': 'No', 'Financiación de la Unión Europea': 'No hay financiación de la Unión Europea', 'Procedimiento de adjudicación': 'Abierto simplificado', 'Tipo de tramitación': 'Ordinaria', 'Método de presentación de ofertas': 'Electrónica', 'Subasta electrónica': 'No', 'Valor estimado sin impuestos': '40.200,00 euros', 'Presupuesto base licitación sin impuestos': '40.200,00 euros', 'Presupuesto base licitación. Importe total': '48.642,00 euros', 'Duración del contrato': '1 meses', 'Fecha y hora límite de presentación de ofertas o solicitudes de participación': '18 de noviembre del  2025 18:00'}\n",
      "Readings: https://contratos-publicos.comunidad.madrid/contrato-publico/2025-0-80-suministro-instalacion-puesta-funcionamiento-cunas-termicas-fototerapia .... \n",
      "\n",
      "{'Tipo de publicación': 'Convocatoria anunciada a licitación', 'Situación': 'En plazo', 'Número de expediente': '2025-0-80', 'Referencia': 'C11152', 'Objeto del contrato': 'Suministro, instalación, y puesta en funcionamiento de cunas térmicas con fototerapia incorporada para el Servicio de UCI Pediátrica del Hospital Universitario 12 de Octubre,', 'Tipo de contrato': 'Suministros', 'Contrato mixto': 'No', 'Código CPV': '33100000-1', 'Contrato/lote reservado': 'No aplica', 'Legislación nacional aplicable': 'Ley 9/2017', 'Sujeto a regulación armonizada': 'No', 'Sistema de contratación': 'No aplica', 'Código NUTS': 'ES300', 'Compra pública de innovación': 'No', 'Financiación de la Unión Europea': 'No hay financiación de la Unión Europea', 'Procedimiento de adjudicación': 'Abierto simplificado', 'Tipo de tramitación': 'Ordinaria', 'Método de presentación de ofertas': 'Electrónica', 'Subasta electrónica': 'No', 'Valor estimado sin impuestos': '137.750,00 euros', 'Presupuesto base licitación sin impuestos': '137.750,00 euros', 'Presupuesto base licitación. Importe total': '166.677,50 euros', 'Duración del contrato': '1 meses', 'Fecha y hora límite de presentación de ofertas o solicitudes de participación': '18 de noviembre del  2025 18:00'}\n",
      "Readings: https://contratos-publicos.comunidad.madrid/contrato-publico/acuerdo-marco-servicio-asistencia-tecnica-materia-seguridad-salud-proyectos-0 .... \n",
      "\n",
      "{'Tipo de publicación': 'Convocatoria anunciada a licitación', 'Situación': 'En plazo', 'Número de expediente': 'A/SER-011758/2025', 'Referencia': 'C10780', 'Objeto del contrato': 'Acuerdo marco para el servicio de asistencia técnica en materia de seguridad y salud de proyectos y obras de la D.G. de Infraestructuras y Servicios de la Consejería de Educación, Ciencia y Universidades', 'Tipo de contrato': 'Servicios', 'Contrato mixto': 'No', 'Código CPV': '71317200-5', 'Contrato/lote reservado': 'No aplica', 'Legislación nacional aplicable': 'Ley 9/2017', 'Sujeto a regulación armonizada': 'Sí', 'Sistema de contratación': 'Establecimiento de un acuerdo marco', 'Código NUTS': 'ES300', 'Compra pública de innovación': 'No', 'Financiación de la Unión Europea': 'No hay financiación de la Unión Europea', 'Procedimiento de adjudicación': 'Abierto', 'Tipo de tramitación': 'Ordinaria', 'Método de presentación de ofertas': 'Electrónica', 'Subasta electrónica': 'No', 'Valor estimado sin impuestos': '826.446,26 euros', 'Presupuesto base licitación sin impuestos': '413.223,13 euros', 'Duración del contrato': '2 años', 'Fecha y hora límite de presentación de ofertas o solicitudes de participación': '7 de noviembre del  2025 23:59'}\n"
     ]
    }
   ],
   "source": [
    "for i in contract_urls[0:3]:\n",
    "    print(f'Readings: {i} .... ')\n",
    "    print()\n",
    "    print(parse_convocatoria_from_url(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c123ff3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tipo de publicación': 'Convocatoria anunciada a licitación',\n",
       " 'Situación': 'En plazo',\n",
       " 'Número de expediente': '2025000044',\n",
       " 'Referencia': 'C10616',\n",
       " 'Identificador del expediente en TED': 'a541a1eb-aef4-44e1-864d-7ffabe4d144c',\n",
       " 'Objeto del contrato': 'Adquisición de dos Torres de Laparoscopia con destino a los Servicios de Ginecología y Cirugía Cardíaca del Hospital Universitario Ramón y Cajal.',\n",
       " 'Tipo de contrato': 'Suministros',\n",
       " 'Contrato mixto': 'No',\n",
       " 'Código CPV': '33100000-1',\n",
       " 'Contrato/lote reservado': 'No aplica',\n",
       " 'Legislación nacional aplicable': 'Ley 9/2017',\n",
       " 'Sujeto a regulación armonizada': 'Sí',\n",
       " 'Sistema de contratación': 'No aplica',\n",
       " 'Código NUTS': 'ES300',\n",
       " 'Compra pública de innovación': 'No',\n",
       " 'Financiación de la Unión Europea': 'No hay financiación de la Unión Europea',\n",
       " 'Procedimiento de adjudicación': 'Abierto',\n",
       " 'Tipo de tramitación': 'Ordinaria',\n",
       " 'Método de presentación de ofertas': 'Electrónica',\n",
       " 'Subasta electrónica': 'No',\n",
       " 'Valor estimado sin impuestos': '235.122,56 euros',\n",
       " 'Presupuesto base licitación sin impuestos': '235.122,56 euros',\n",
       " 'Presupuesto base licitación. Importe total': '284.498,30 euros',\n",
       " 'Duración del contrato': '2 meses',\n",
       " 'Fecha y hora límite de presentación de ofertas o solicitudes de participación': '27 de octubre del  2025 18:00'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_convocatoria_from_url(contract_urls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881c697b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc48de6e",
   "metadata": {},
   "source": [
    "## Question 2: Chunking strategy \n",
    "\n",
    "There are two types of documents.Each type reqiures a different chunking strategy\n",
    "\n",
    "**Contracts** \n",
    "\n",
    "Each contract record already comes as structured JSON (publication type, procedure, CPV, amounts, deadlines, etc.), so no chunking is needed. The strategy is then to treat each contract as a single atomic document — one JSON object per contract. Use the contract ID as the primary key. \n",
    "\n",
    "**PDF Attachments** \n",
    "\n",
    "The goal is to extract structured and textual information from the PDF pliegos for Q&A, risk detection, and table reconstruction. For this a hybrid “page-aware + table-aware” strategy to handle text sections and tables efficiently. In particular, split by sections or headings, not blindly by page count. Some rules that can apply: \n",
    "\n",
    "- Use 2–3 pages per chunk (CHUNK_SIZE=2–3, STEP=1) to keep semantic context.\n",
    "\n",
    "- Always include a small line overlap (2–3 lines) between consecutive chunks.\n",
    "\n",
    "- Define chunks around ~12,000 characters (≈1–1.5k tokens).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f18c220",
   "metadata": {},
   "source": [
    "## Question 3: Database\n",
    "\n",
    "The characteristics of the project suggest using persisting storage. It will be used for the following \n",
    "\n",
    "- Raw PDFs/HTML and text/LLM outputs (expensive to recreate).\n",
    "\n",
    "- Stable embeddings and chunk boundaries.\n",
    "\n",
    "- Any curated field you’ll query later (contract value, duration, lots).\n",
    "\n",
    "- Run logs & costs (for observability and regression checks).\n",
    "\n",
    "In addition, there will be vectors searches and these vectors could be stored in a database like Qdrant. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a65d0e",
   "metadata": {},
   "source": [
    "# PART 2: BUILD A SUMMARY AGENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a3360f",
   "metadata": {},
   "source": [
    "## Question 5 \n",
    "\n",
    "Build an agent that does a couple of things:\n",
    "\n",
    "- Use the `Fetch Web Page` tool to get the content of the page\n",
    "\n",
    "- Use the `Save Summary` tool to save the summary\n",
    "\n",
    "**Rationale of the Agent** \n",
    "\n",
    "The agent is composed of three key layers working together: Pydantic Agent, the Tools and the Pydantic Models\n",
    "\n",
    "- **Pydantic Agent** -> It is the controller that receives natural language requests from the user. Interprets the language and decides what tool to use.\n",
    "\n",
    "- **Tool(s)** -> These are functions that actually create the capabilities of the agents.\n",
    "\n",
    "- **Models** -> Define exactly what each tool returns. They ensure the agent provides a structured and coherent output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eb41fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import datetime as dt\n",
    "from typing import Optional, Literal\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pydantic import BaseModel, HttpUrl, Field\n",
    "from openai import OpenAI\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "\n",
    "# ---------------- Models ----------------\n",
    "\n",
    "class FetchOutput(BaseModel):\n",
    "    url: HttpUrl\n",
    "    title: Optional[str]\n",
    "    text: str\n",
    "    fetched_at: dt.datetime = Field(default_factory=lambda: dt.datetime.now(dt.timezone.utc))\n",
    "\n",
    "class SummaryOutput(BaseModel):\n",
    "    url: HttpUrl\n",
    "    title: Optional[str]\n",
    "    summary: str\n",
    "    saved_to: str\n",
    "    created_at: dt.datetime = Field(default_factory=lambda: dt.datetime.now(dt.timezone.utc))\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "\n",
    "def _slug_from_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Build a filesystem-safe slug from the URL:\n",
    "    - use the last path component if present; otherwise use the hostname\n",
    "    - strip query/fragment\n",
    "    - lowercase and keep only [a-z0-9_-], replacing others with '-'\n",
    "    \"\"\"\n",
    "    p = urlparse(url)\n",
    "    last = (p.path or \"\").strip(\"/\").split(\"/\")[-1]\n",
    "    base = last or p.netloc\n",
    "    base = base.split(\"?\")[0].split(\"#\")[0]\n",
    "    slug = re.sub(r\"[^a-zA-Z0-9_-]+\", \"-\", base).strip(\"-\").lower()\n",
    "    if not slug:\n",
    "        slug = re.sub(r\"[^a-zA-Z0-9]+\", \"-\", p.netloc).strip(\"-\").lower() or \"page\"\n",
    "    return slug\n",
    "\n",
    "def _fetch_text_and_title(url: str, timeout=(5, 20)) -> tuple[str, Optional[str]]:\n",
    "    r = requests.get(url, headers={\"User-Agent\": \"WebFetch/0.3\"}, timeout=timeout, allow_redirects=True)\n",
    "    r.raise_for_status()\n",
    "    html = r.text\n",
    "\n",
    "    # Fast path for Wikipedia (REST plain)\n",
    "    if \"wikipedia.org\" in r.url:\n",
    "        try:\n",
    "            slug = r.url.rsplit(\"/\", 1)[-1]\n",
    "            rr = requests.get(\n",
    "                f\"https://en.wikipedia.org/api/rest_v1/page/plain/{slug}\",\n",
    "                headers={\"User-Agent\": \"WebFetch/0.3\"},\n",
    "                timeout=(5, 15),\n",
    "            )\n",
    "            rr.raise_for_status()\n",
    "            text = rr.text\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            title = soup.title.string.strip() if (soup.title and soup.title.string) else None\n",
    "            return text, title\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
    "        tag.decompose()\n",
    "    title = soup.title.string.strip() if (soup.title and soup.title.string) else None\n",
    "    text = \"\\n\".join(t.strip() for t in soup.stripped_strings)\n",
    "    return text, title\n",
    "\n",
    "# ---------------- Tool: fetch_content ----------------\n",
    "\n",
    "def fetch_content(url: HttpUrl) -> FetchOutput:\n",
    "    \"\"\"Fetch full text from a webpage.\"\"\"\n",
    "    resp = requests.get(url, headers={\"User-Agent\": \"WebFetch/0.1\"}, timeout=20)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    title = soup.title.string.strip() if soup.title and soup.title.string else None\n",
    "    text = \"\\n\".join(t.strip() for t in soup.stripped_strings)\n",
    "\n",
    "    return FetchOutput(url=url, title=title, text=text)\n",
    "\n",
    "# ---------------- Tool: save_summary ----------------\n",
    "\n",
    "_SUMMARY_KEYS_SEEN: set[tuple[str, str, str]] = set()  # (url, style, saved_path)\n",
    "\n",
    "def save_summary(\n",
    "    url: HttpUrl,\n",
    "    style: Literal[\"bullet\", \"exec\"] = \"bullet\",\n",
    "    out_dir: str = \"summaries\",\n",
    "    model_name: Optional[str] = None,\n",
    ") -> SummaryOutput:\n",
    "    \"\"\"\n",
    "    One-shot: fetch full text from URL, summarize with OpenAI, save to a single JSON file\n",
    "    named {slug}_summary.json under `out_dir`, and return the saved path.\n",
    "\n",
    "    Idempotent within the process: avoids duplicate summaries for the same (url, style, out_dir/slug_summary.json).\n",
    "    \"\"\"\n",
    "    # Fetch content\n",
    "    html_clean, title = _fetch_text_and_title(str(url))\n",
    "    text = \" \".join(BeautifulSoup(html_clean, \"html.parser\").stripped_strings)\n",
    "\n",
    "\n",
    "    # Compose filename\n",
    "    slug = _slug_from_url(str(url))\n",
    "    os.makedirs(out_dir or \".\", exist_ok=True)\n",
    "    saved_path = os.path.join(out_dir, f\"{slug}_summary.json\")\n",
    "\n",
    "    key = (str(url), style, saved_path)\n",
    "    if key in _SUMMARY_KEYS_SEEN:\n",
    "        # Already summarized in this run\n",
    "        return SummaryOutput(url=url, title=title, summary=\"(already summarized in this run)\", saved_to=saved_path)\n",
    "\n",
    "    # OpenAI client\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"OPENAI_API_KEY must be set\")\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    model = model_name or os.getenv(\"AGENT_MODEL\", \"gpt-4o-mini\")\n",
    "\n",
    "    style_instr = (\n",
    "        \"Return 5–8 concise bullets with key facts, numbers, and dates. End with a one-sentence takeaway.\"\n",
    "        if style == \"bullet\"\n",
    "        else \"Write a tight 2–3 paragraph executive summary followed by 3 concrete action items.\"\n",
    "    )\n",
    "\n",
    "    # Truncate long inputs (latency/cost control)\n",
    "    max_chars = 120_000\n",
    "    body = text if len(text) <= max_chars else (text[:max_chars] + \"\\n...[truncated]\")\n",
    "\n",
    "    prompt = f\"Title: {title or 'N/A'}\\nURL: {url}\\n\\n{style_instr}\\n\\nCONTENT:\\n{body}\"\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.2,\n",
    "        max_tokens=500,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a precise summarizer. Stay faithful to the source.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    summary = (resp.choices[0].message.content or \"\").strip()\n",
    "\n",
    "    # Save a single JSON file named {slug}_summary.json\n",
    "    record = {\n",
    "        \"url\": str(url),\n",
    "        \"title\": title,\n",
    "        \"summary\": summary,\n",
    "        \"model\": model,\n",
    "        \"created_at\": dt.datetime.now(dt.timezone.utc).isoformat(),\n",
    "        \"saved_to\": saved_path,\n",
    "    }\n",
    "    with open(saved_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(record, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    _SUMMARY_KEYS_SEEN.add(key)\n",
    "    return SummaryOutput(url=url, title=title, summary=summary, saved_to=saved_path)\n",
    "\n",
    "# ---------------- Agent ----------------\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You have two tools:\n",
    "- fetch_content(url): fetch full webpage content.\n",
    "- save_summary(url, style?, out_dir?, model_name?): fetch, summarize, and save to {slug}_summary.json.\n",
    "\n",
    "Behavioral rules:\n",
    "• If the user asks to fetch AND save a summary in one request, call ONLY save_summary ONCE.\n",
    "• Never call save_summary more than once per user request.\n",
    "• If you already produced a summary for the same (url, style, out_dir) in this run, do not run it again.\n",
    "• Return only the final tool result for the user’s request.\n",
    "• Always output the saved summary details clearly, including:\n",
    "    - url\n",
    "    - title\n",
    "    - summary\n",
    "    - saved_to path (should end with {slug}_summary.json)\n",
    "    - timestamp\n",
    "\"\"\".strip()\n",
    "\n",
    "agent = Agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[fetch_content, save_summary],\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a311c5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[index_links] Error procesando https://en.wikipedia.org/wiki/Capybara: Embedding dimension mismatch: index has 1536, new has 3072\n",
      "[index_links] Error procesando https://en.wikipedia.org/wiki/Lesser_capybara: Embedding dimension mismatch: index has 1536, new has 3072\n",
      "[index_links] Error procesando https://en.wikipedia.org/wiki/Hydrochoerus: Embedding dimension mismatch: index has 1536, new has 3072\n",
      "[index_links] Error procesando https://en.wikipedia.org/wiki/Neochoerus: Embedding dimension mismatch: index has 1536, new has 3072\n",
      "[index_links] Error procesando https://en.wikipedia.org/wiki/Caviodon: Embedding dimension mismatch: index has 1536, new has 3072\n",
      "[index_links] Error procesando https://en.wikipedia.org/wiki/Neochoerus_aesopi: Embedding dimension mismatch: index has 1536, new has 3072\n",
      "[VectorIndex] WARNING: index at 'index' was built with 'text-embedding-3-large', but 'text-embedding-ada-002' was requested. Adopting stored model to avoid mismatch.\n"
     ]
    }
   ],
   "source": [
    "res = await agent.run(\"What is this page about https://en.wikipedia.org/wiki/Capybara and save a bullet summary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9295b525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentRunResult(data='Here is the summary of the Wikipedia page about Capybaras:\\n\\n- **URL**: [Capybara - Wikipedia](https://en.wikipedia.org/wiki/Capybara)\\n- **Title**: Capybara - Wikipedia\\n- **Summary**:\\n  - The capybara (Hydrochoerus hydrochaeris) is the largest living rodent, native to South America, and can weigh between 35 to 66 kg (77 to 146 lb).\\n  - Adult capybaras typically measure 106 to 134 cm (3.48 to 4.40 ft) in length and stand 50 to 62 cm (20 to 24 in) tall at the withers.\\n  - They are highly social animals, often found in groups of 10-20, but can gather in larger groups of up to 100 during the dry season.\\n  - Capybaras are herbivores, primarily grazing on grasses and aquatic plants, and exhibit autocoprophagy, consuming their own feces to aid digestion.\\n  - Their maximum lifespan is 8 to 10 years in captivity, but they usually live only about four years in the wild due to predation.\\n  - Capybaras are not considered threatened, with a stable population across most of their range, although hunting and habitat loss pose risks in some areas.\\n  - They communicate using a variety of sounds, including barks, chirps, and whistles, and have a unique mating behavior that occurs in water.\\n  - Capybaras have gained popularity in culture and social media, often depicted in memes and as symbols of class struggle in Argentina.\\n- **Saved to**: summaries/capybara_summary.json\\n- **Timestamp**: 2025-10-27T04:59:01.700150Z')\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809c8c03",
   "metadata": {},
   "source": [
    "## Question 6. Index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c46001c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "import datetime as dt\n",
    "from typing import Optional, Literal, List, Dict, Tuple, Iterable\n",
    "from urllib.parse import urlparse, quote, unquote\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pydantic import BaseModel, HttpUrl, Field\n",
    "from openai import OpenAI\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Optional BM25 (nice boost for lexical exact matches)\n",
    "try:\n",
    "    from rank_bm25 import BM25Okapi\n",
    "    USE_BM25 = True\n",
    "except Exception:\n",
    "    BM25Okapi = None\n",
    "    USE_BM25 = False\n",
    "\n",
    "# ---------------- Models ----------------\n",
    "\n",
    "class FetchOutput(BaseModel):\n",
    "    url: HttpUrl\n",
    "    title: Optional[str]\n",
    "    text: str\n",
    "    fetched_at: dt.datetime = Field(default_factory=lambda: dt.datetime.now(dt.timezone.utc))\n",
    "\n",
    "class SummaryOutput(BaseModel):\n",
    "    url: HttpUrl\n",
    "    title: Optional[str]\n",
    "    summary: str\n",
    "    saved_to: str\n",
    "    created_at: dt.datetime = Field(default_factory=lambda: dt.datetime.now(dt.timezone.utc))\n",
    "\n",
    "class IndexStats(BaseModel):\n",
    "    added_docs: int\n",
    "    total_docs: int\n",
    "    index_dir: str\n",
    "    embedding_model: str\n",
    "    bm25_enabled: bool\n",
    "    updated_at: dt.datetime = Field(default_factory=lambda: dt.datetime.now(dt.timezone.utc))\n",
    "\n",
    "\n",
    "class IndexLinksParams(BaseModel):\n",
    "    urls: List[str]\n",
    "    index_dir: str = \"index\"\n",
    "    embed_model: str = \"text-embedding-ada-002\" \n",
    "    chunk_tokens: int = 350\n",
    "    overlap_tokens: int = 50\n",
    "    connect_timeout: int = Field(5, ge=1)\n",
    "    read_timeout: int = Field(20, ge=1)\n",
    "\n",
    "\n",
    "class SearchHit(BaseModel):\n",
    "    url: HttpUrl\n",
    "    title: Optional[str]\n",
    "    chunk_id: str\n",
    "    score: float\n",
    "    snippet: str\n",
    "\n",
    "class AnswerOutput(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "    citations: List[Dict[str, str]]  # {\"url\":..., \"title\":..., \"chunk_id\":...}\n",
    "    used_model: str\n",
    "    generated_at: dt.datetime = Field(default_factory=lambda: dt.datetime.now(dt.timezone.utc))\n",
    "\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "\n",
    "def _slug_from_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Build a filesystem-safe slug from the URL:\n",
    "    - use the last path component if present; otherwise use the hostname\n",
    "    - strip query/fragment\n",
    "    - lowercase and keep only [a-z0-9_-], replacing others with '-'\n",
    "    \"\"\"\n",
    "    p = urlparse(url)\n",
    "    last = (p.path or \"\").strip(\"/\").split(\"/\")[-1]\n",
    "    base = last or p.netloc\n",
    "    base = base.split(\"?\")[0].split(\"#\")[0]\n",
    "    slug = re.sub(r\"[^a-zA-Z0-9_-]+\", \"-\", base).strip(\"-\").lower()\n",
    "    if not slug:\n",
    "        slug = re.sub(r\"[^a-zA-Z0-9]+\", \"-\", p.netloc).strip(\"-\").lower() or \"page\"\n",
    "    return slug\n",
    "\n",
    "def _atomic_write(path: str, data: bytes) -> None:\n",
    "    \"\"\"\n",
    "    Atomically write bytes to `path`. Ensures parent directory exists.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(path) or \".\", exist_ok=True)\n",
    "    tmp = f\"{path}.tmp\"\n",
    "    with open(tmp, \"wb\") as f:\n",
    "        f.write(data)\n",
    "        f.flush()\n",
    "        os.fsync(f.fileno())\n",
    "    os.replace(tmp, path)\n",
    "\n",
    "def _read_jsonl(path: str) -> Iterable[dict]:\n",
    "    if not os.path.exists(path):\n",
    "        return []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                yield json.loads(line)\n",
    "\n",
    "def _append_jsonl(path: str, records: List[dict]) -> None:\n",
    "    os.makedirs(os.path.dirname(path) or \".\", exist_ok=True)\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        for r in records:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def _hash_id(*parts: str) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    for p in parts:\n",
    "        h.update(p.encode(\"utf-8\", errors=\"ignore\"))\n",
    "    return h.hexdigest()[:16]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _clean_text(html: str) -> Tuple[str, Optional[str]]:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    # Try to focus on the main article content\n",
    "    main = soup.find(attrs={\"role\": \"main\"}) or soup.find(\"main\") or soup.find(\"article\")\n",
    "    root = main or soup\n",
    "\n",
    "    # Remove obvious non-content\n",
    "    for tag in root([\"script\", \"style\", \"noscript\", \"header\", \"footer\", \"nav\", \"aside\", \"form\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    title_el = soup.title\n",
    "    title = title_el.string.strip() if (title_el and title_el.string) else None\n",
    "\n",
    "    # Keep headings to preserve section boundaries\n",
    "    texts: List[str] = []\n",
    "    for el in root.find_all([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\",\"p\",\"li\",\"table\"]):\n",
    "        t = el.get_text(\" \", strip=True)\n",
    "        if not t:\n",
    "            continue\n",
    "        # Add simple heading markers for later section-aware chunking\n",
    "        if el.name in {\"h1\",\"h2\",\"h3\"}:\n",
    "            texts.append(f\"\\n\\n## {t}\\n\")\n",
    "        else:\n",
    "            texts.append(t)\n",
    "\n",
    "    text = \"\\n\".join(texts)\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text, title\n",
    "\n",
    "\n",
    "def _fetch_text_and_title(url: str, timeout=(5, 20)) -> tuple[str, Optional[str]]:\n",
    "    r = requests.get(url, headers={\"User-Agent\": \"WebFetch/0.4\"}, timeout=timeout, allow_redirects=True)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    # Fast path for Wikipedia (REST \"plain\" or \"mobile-html\")\n",
    "    if \"wikipedia.org\" in r.url:\n",
    "        slug = r.url.rsplit(\"/\", 1)[-1]\n",
    "        # try /page/plain (no UI, just article text)\n",
    "        try:\n",
    "            rr = requests.get(\n",
    "                f\"https://en.wikipedia.org/api/rest_v1/page/plain/{slug}\",\n",
    "                headers={\"User-Agent\": \"WebFetch/0.4\"},\n",
    "                timeout=(5, 15),\n",
    "            )\n",
    "            rr.raise_for_status()\n",
    "            title = BeautifulSoup(r.text, \"html.parser\").title\n",
    "            title = title.string.strip() if (title and title.string) else None\n",
    "            return rr.text, title\n",
    "        except Exception:\n",
    "            pass\n",
    "        # fallback: mobile-html (less chrome)\n",
    "        try:\n",
    "            rr = requests.get(\n",
    "                f\"https://en.wikipedia.org/api/rest_v1/page/mobile-html/{slug}\",\n",
    "                headers={\"User-Agent\": \"WebFetch/0.4\"},\n",
    "                timeout=(5, 15),\n",
    "            )\n",
    "            rr.raise_for_status()\n",
    "            text, title = _clean_text(rr.text)\n",
    "            return text, title\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Generic sites\n",
    "    return _clean_text(r.text)\n",
    "\n",
    "\n",
    "\n",
    "def _tokenize(text: str) -> List[str]:\n",
    "    return re.findall(r\"[A-Za-z0-9_]+\", (text or \"\").lower())\n",
    "\n",
    "\n",
    "def _build_snippet(full_text: str, query: str, window: int = 220) -> str:\n",
    "    txt = (full_text or \"\").replace(\"\\n\", \" \")\n",
    "    if not txt:\n",
    "        return \"\"\n",
    "    q_terms = [w for w in _tokenize(query) if len(w) > 2]\n",
    "    if not q_terms:\n",
    "        return txt[:window] + (\"…\" if len(txt) > window else \"\")\n",
    "    # center around first matching term\n",
    "    locs = []\n",
    "    low = txt.lower()\n",
    "    for t in q_terms:\n",
    "        m = re.search(rf\"\\b{re.escape(t)}\\b\", low)\n",
    "        if m:\n",
    "            locs.append(m.start())\n",
    "    if not locs:\n",
    "        return txt[:window] + (\"…\" if len(txt) > window else \"\")\n",
    "    pos = min(locs)\n",
    "    half = window // 2\n",
    "    start = max(0, pos - half)\n",
    "    end = min(len(txt), start + window)\n",
    "    snippet = txt[start:end]\n",
    "    if start > 0:\n",
    "        snippet = \"…\" + snippet\n",
    "    if end < len(txt):\n",
    "        snippet = snippet + \"…\"\n",
    "    return snippet\n",
    "\n",
    "\n",
    "def _mmr_rerank(cand_idx: List[int], query_vec: np.ndarray, emb_matrix: np.ndarray, k: int, alpha: float = 0.7) -> List[int]:\n",
    "    \"\"\"\n",
    "    MMR over the candidate pool.\n",
    "    cand_idx: list of GLOBAL row ids (len = C)\n",
    "    emb_matrix: full [N, D] or already sliced; we’ll slice here for clarity\n",
    "    Returns a list of GLOBAL row ids in selected order (length <= k).\n",
    "    \"\"\"\n",
    "    C = len(cand_idx)\n",
    "    if C == 0:\n",
    "        return []\n",
    "    if C == 1:\n",
    "        return cand_idx[:1]\n",
    "\n",
    "    # Build local matrix [C, D]\n",
    "    M = emb_matrix[cand_idx].astype(np.float32)\n",
    "    # Normalize\n",
    "    M /= (np.linalg.norm(M, axis=1, keepdims=True) + 1e-9)\n",
    "    Q = query_vec.astype(np.float32)\n",
    "    Q /= (np.linalg.norm(Q) + 1e-9)\n",
    "\n",
    "    sim_to_q = M @ Q  # [C]\n",
    "\n",
    "    # Work strictly in LOCAL positions [0..C-1]\n",
    "    remaining = list(range(C))\n",
    "    selected_local: List[int] = []\n",
    "\n",
    "    while remaining and len(selected_local) < k:\n",
    "        if not selected_local:\n",
    "            j_in_rem = int(np.argmax(sim_to_q[remaining]))\n",
    "            selected_local.append(remaining.pop(j_in_rem))\n",
    "            continue\n",
    "\n",
    "        # Similarity to already selected\n",
    "        S = M[selected_local] @ M.T              # [|S|, C]\n",
    "        max_sim_to_S = np.max(S[:, remaining], axis=0)  # [|remaining|]\n",
    "        mmr_scores = alpha * sim_to_q[remaining] - (1.0 - alpha) * max_sim_to_S\n",
    "        j_in_rem = int(np.argmax(mmr_scores))\n",
    "        selected_local.append(remaining.pop(j_in_rem))\n",
    "\n",
    "    # Map back to GLOBAL row ids\n",
    "    return [cand_idx[i] for i in selected_local]\n",
    "\n",
    "\n",
    "def _chunk_text(text: str, target_tokens: int = 350, overlap_tokens: int = 50) -> List[str]:\n",
    "    if not text:\n",
    "        return []\n",
    "    # split by our synthetic headings to keep sections cohesive\n",
    "    sections = re.split(r\"\\n{2,}## \", text)\n",
    "    sections = [s.strip() for s in sections if s and s.strip()]\n",
    "    chunks: List[str] = []\n",
    "    # token ≈ 0.75 words\n",
    "    chunk_words = max(50, int(target_tokens / 0.75))\n",
    "    overlap_words = int(overlap_tokens / 0.75)\n",
    "\n",
    "    for sec in sections:\n",
    "        words = sec.split()\n",
    "        if not words:\n",
    "            continue\n",
    "        start = 0\n",
    "        while start < len(words):\n",
    "            end = min(len(words), start + chunk_words)\n",
    "            ch = \" \".join(words[start:end])\n",
    "            if len(ch.split()) >= 12:  # avoid tiny fragments\n",
    "                chunks.append(ch)\n",
    "            if end == len(words):\n",
    "                break\n",
    "            start = max(end - overlap_words, start + 1)\n",
    "    return chunks\n",
    "\n",
    "def _index_links_impl(\n",
    "    urls: List[str],\n",
    "    index_dir: str = \"index\",\n",
    "    embed_model: str = \"text-embedding-ada-002\",\n",
    "    chunk_tokens: int = 350,\n",
    "    overlap_tokens: int = 50,\n",
    "    connect_timeout: int = 5,\n",
    "    read_timeout: int = 20,\n",
    ") -> IndexStats:\n",
    "    \"\"\"\n",
    "    Implementación principal que realiza:\n",
    "    1. Descarga y limpieza del contenido.\n",
    "    2. Segmentación (chunking).\n",
    "    3. Cálculo de embeddings.\n",
    "    4. Escritura incremental en el índice persistente.\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"OPENAI_API_KEY must be set\")\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    idx = VectorIndex(index_dir=index_dir, embedding_model=embed_model)\n",
    "    added = 0\n",
    "\n",
    "    for u in urls:\n",
    "        try:\n",
    "            if not re.match(r\"^https?://\", u):\n",
    "                print(f\"[WARN] URL inválida: {u}\")\n",
    "                continue\n",
    "            text, title = _fetch_text_and_title(u, timeout=(connect_timeout, read_timeout))\n",
    "            chunks = _chunk_text(text, target_tokens=chunk_tokens, overlap_tokens=overlap_tokens)\n",
    "            chunks = [c for c in chunks if len(c.split()) >= 5]\n",
    "            if not chunks:\n",
    "                 print(f\"[index_links] No usable chunks for {u} (after filtering).\")\n",
    "                 continue\n",
    "            added += idx.add_document(client, u, title, chunks)\n",
    "        except Exception as e:\n",
    "            print(f\"[index_links] Error procesando {u}: {e}\")\n",
    "\n",
    "    stats = IndexStats(\n",
    "        added_docs=added,\n",
    "        total_docs=idx.get_total(),\n",
    "        index_dir=index_dir,\n",
    "        embedding_model=embed_model,\n",
    "        bm25_enabled=USE_BM25,\n",
    "    )\n",
    "\n",
    "    os.makedirs(index_dir, exist_ok=True)\n",
    "    _atomic_write(\n",
    "        os.path.join(index_dir, \"stats.json\"),\n",
    "        json.dumps(stats.model_dump(mode='json'), ensure_ascii=False, indent=2).encode(\"utf-8\"),\n",
    "    )\n",
    "    return stats\n",
    "\n",
    "\n",
    "def index_links(params: IndexLinksParams) -> IndexStats:\n",
    "    \"\"\"\n",
    "    Wrapper JSON-schema-friendly (para usar como tool del agente).\n",
    "    Convierte el modelo pydantic en argumentos posicionales seguros.\n",
    "    \"\"\"\n",
    "    return _index_links_impl(**params.model_dump())\n",
    "\n",
    "\n",
    "# ---------------- Embeddings + Index storage ----------------\n",
    "\n",
    "class VectorIndex:\n",
    "    \"\"\"\n",
    "    Persistent index for embeddings + metadata.\n",
    "\n",
    "    Files (under index_dir):\n",
    "      - meta.jsonl         (per-chunk metadata: {chunk_id,url,title,order,created_at})\n",
    "      - embeddings.npy     (float32 matrix [N x D])\n",
    "      - embeddings.npy.shape  (json sidecar: {\"n\": N, \"d\": D})\n",
    "      - docstore.jsonl     (per-chunk text payload: {chunk_id,text})\n",
    "      - bm25_tokens.jsonl  (optional; tokens list per chunk for BM25)\n",
    "      - stats.json         (IndexStats)\n",
    "    \"\"\"\n",
    "    KNOWN_DIMS = {\n",
    "        \"text-embedding-3-large\": 3072,\n",
    "        \"text-embedding-3-small\": 1536,\n",
    "        \"text-embedding-ada-002\": 1536,\n",
    "    }\n",
    "\n",
    "    def __init__(self, index_dir: str = \"index\", embedding_model: str = \"text-embedding-ada-002\"):\n",
    "        self.index_dir = index_dir\n",
    "        self.meta_path = os.path.join(index_dir, \"meta.jsonl\")\n",
    "        self.vec_path = os.path.join(index_dir, \"embeddings.npy\")\n",
    "        self.shape_path = self.vec_path + \".shape\"\n",
    "        self.doc_path = os.path.join(index_dir, \"docstore.jsonl\")\n",
    "        self.bm25_path = os.path.join(index_dir, \"bm25_tokens.jsonl\")\n",
    "        self.stats_path = os.path.join(index_dir, \"stats.json\")\n",
    "\n",
    "        self.embedding_model = embedding_model\n",
    "        self._meta: List[dict] = list(_read_jsonl(self.meta_path))\n",
    "        self._docs: Dict[str, str] = {d[\"chunk_id\"]: d[\"text\"] for d in _read_jsonl(self.doc_path)}\n",
    "        self._bm25_tokens: Dict[str, List[str]] = (\n",
    "            {d[\"chunk_id\"]: d[\"tokens\"] for d in _read_jsonl(self.bm25_path)} if USE_BM25 else {}\n",
    "        )\n",
    "\n",
    "        # --- Read stored metadata to reconcile model + dims ---\n",
    "        stored_model = None\n",
    "        stored_dim = None\n",
    "        if os.path.exists(self.stats_path):\n",
    "            try:\n",
    "                _stats = json.loads(open(self.stats_path, \"r\", encoding=\"utf-8\").read())\n",
    "                stored_model = _stats.get(\"embedding_model\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        if os.path.exists(self.shape_path):\n",
    "            try:\n",
    "                shp = json.loads(open(self.shape_path, \"r\", encoding=\"utf-8\").read())\n",
    "                stored_dim = int(shp.get(\"d\")) if \"d\" in shp else None\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # If index already exists with a different model, ADOPT the stored model to avoid mismatch.\n",
    "        if stored_model and stored_model != self.embedding_model:\n",
    "            print(\n",
    "                f\"[VectorIndex] WARNING: index at '{self.index_dir}' was built with '{stored_model}', \"\n",
    "                f\"but '{self.embedding_model}' was requested. Adopting stored model.\"\n",
    "            )\n",
    "            self.embedding_model = stored_model\n",
    "\n",
    "        # Set expected dimension for the (possibly adopted) model\n",
    "        self._dim_expected: Optional[int] = self.KNOWN_DIMS.get(self.embedding_model)\n",
    "\n",
    "        # If shape sidecar missing but vec file exists, infer N,D using expected dim\n",
    "        self._emb: Optional[np.memmap] = None\n",
    "        if os.path.exists(self.vec_path):\n",
    "            n = d = None\n",
    "            if os.path.exists(self.shape_path):\n",
    "                try:\n",
    "                    shp = json.loads(open(self.shape_path, \"r\", encoding=\"utf-8\").read())\n",
    "                    n, d = int(shp.get(\"n\", 0)), int(shp.get(\"d\", 0))\n",
    "                except Exception:\n",
    "                    n = d = None\n",
    "            if not n or not d:\n",
    "                if self._dim_expected:\n",
    "                    size_bytes = os.path.getsize(self.vec_path)\n",
    "                    if size_bytes > 0 and size_bytes % (4 * self._dim_expected) == 0:\n",
    "                        n = size_bytes // (4 * self._dim_expected)\n",
    "                        d = self._dim_expected\n",
    "                        with open(self.shape_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                            json.dump({\"n\": int(n), \"d\": int(d)}, f)\n",
    "            if n and d:\n",
    "                # Validate against expected (if both known)\n",
    "                if stored_dim and self._dim_expected and stored_dim != self._dim_expected:\n",
    "                    raise ValueError(\n",
    "                        f\"Embedding dimension mismatch: index has dim={stored_dim}, \"\n",
    "                        f\"but model '{self.embedding_model}' expects {self._dim_expected}. \"\n",
    "                        f\"Rebuild the index or remove '{self.index_dir}'.\"\n",
    "                    )\n",
    "                self._emb = np.memmap(self.vec_path, dtype=np.float32, mode=\"r\", shape=(n, d))\n",
    "\n",
    "        # Build BM25 index in-memory if enabled\n",
    "        self._bm25 = None\n",
    "        if USE_BM25 and self._bm25_tokens:\n",
    "            corpus = [self._bm25_tokens[cid] for cid in self._iter_chunk_ids()]\n",
    "            if corpus and BM25Okapi:\n",
    "                self._bm25 = BM25Okapi(corpus)\n",
    "\n",
    "    def _iter_chunk_ids(self) -> Iterable[str]:\n",
    "        for m in self._meta:\n",
    "            yield m[\"chunk_id\"]\n",
    "\n",
    "    def _append(self, records: List[dict], embeds: np.ndarray, docs: List[dict], bm25_tokens: Optional[List[dict]]) -> None:\n",
    "        # Sanity\n",
    "        if embeds is None or embeds.ndim != 2 or embeds.shape[0] == 0:\n",
    "            return\n",
    "        if len(records) != embeds.shape[0] or len(docs) != embeds.shape[0]:\n",
    "            raise ValueError(\"records/docs count must match number of embedding rows\")\n",
    "\n",
    "        # Append JSONL\n",
    "        _append_jsonl(self.meta_path, records)\n",
    "        _append_jsonl(self.doc_path, docs)\n",
    "        if USE_BM25 and bm25_tokens:\n",
    "            _append_jsonl(self.bm25_path, bm25_tokens)\n",
    "\n",
    "        shape_path = self.vec_path + \".shape\"\n",
    "\n",
    "        if self._emb is None:\n",
    "            # First write\n",
    "            _atomic_write(self.vec_path, embeds.astype(np.float32).tobytes())\n",
    "            _atomic_write(shape_path, json.dumps({\"n\": int(embeds.shape[0]), \"d\": int(embeds.shape[1])}).encode(\"utf-8\"))\n",
    "            self._emb = np.memmap(self.vec_path, dtype=np.float32, mode=\"r\", shape=(embeds.shape[0], embeds.shape[1]))\n",
    "        else:\n",
    "            # Concatenate to existing matrix\n",
    "            if os.path.exists(shape_path):\n",
    "                shp = json.loads(open(shape_path, \"r\", encoding=\"utf-8\").read())\n",
    "                n, d = int(shp[\"n\"]), int(shp[\"d\"])\n",
    "            else:\n",
    "                n, d = self._emb.shape\n",
    "\n",
    "            if embeds.shape[1] != d:\n",
    "                raise ValueError(f\"Embedding dimension mismatch: index has {d}, new has {embeds.shape[1]}\")\n",
    "\n",
    "            new_all = np.empty((n + embeds.shape[0], d), dtype=np.float32)\n",
    "            # read existing into RAM (memmap -> ndarray)\n",
    "            new_all[:n] = np.array(self._emb, dtype=np.float32)\n",
    "            new_all[n:] = embeds.astype(np.float32)\n",
    "\n",
    "            _atomic_write(self.vec_path, new_all.tobytes())\n",
    "            _atomic_write(shape_path, json.dumps({\"n\": int(new_all.shape[0]), \"d\": int(d)}).encode(\"utf-8\"))\n",
    "            self._emb = np.memmap(self.vec_path, dtype=np.float32, mode=\"r\", shape=(new_all.shape[0], d))\n",
    "\n",
    "        # Update in-memory structures\n",
    "        self._meta.extend(records)\n",
    "        self._docs.update({d[\"chunk_id\"]: d[\"text\"] for d in docs})\n",
    "        if USE_BM25 and bm25_tokens:\n",
    "            for rec in bm25_tokens:\n",
    "                self._bm25_tokens[rec[\"chunk_id\"]] = rec[\"tokens\"]\n",
    "\n",
    "        # Refresh BM25 if enabled\n",
    "        if USE_BM25 and BM25Okapi:\n",
    "            corpus = [self._bm25_tokens[cid] for cid in self._iter_chunk_ids()]\n",
    "            self._bm25 = BM25Okapi(corpus) if corpus else None\n",
    "\n",
    "        # Persist stats\n",
    "        stats = IndexStats(\n",
    "            added_docs=len(records),\n",
    "            total_docs=len(self._meta),\n",
    "            index_dir=self.index_dir,\n",
    "            embedding_model=self.embedding_model,\n",
    "            bm25_enabled=USE_BM25,\n",
    "        )\n",
    "        _atomic_write(self.stats_path, json.dumps(stats.model_dump(mode=\"json\"), ensure_ascii=False, indent=2).encode(\"utf-8\"))\n",
    "\n",
    "\n",
    "    # ---------- Public APIs ----------\n",
    "\n",
    "    def contains_url(self, url: str) -> bool:\n",
    "        return any(m[\"url\"] == url for m in self._meta)\n",
    "\n",
    "    def get_total(self) -> int:\n",
    "        return len(self._meta)\n",
    "\n",
    "    def embed_texts(self, client: OpenAI, texts: List[str]) -> np.ndarray:\n",
    "        if not texts:\n",
    "            return np.empty((0, 0), dtype=np.float32)\n",
    "        resp = client.embeddings.create(model=self.embedding_model, input=texts)\n",
    "        data = getattr(resp, \"data\", None) or []\n",
    "        if not data:\n",
    "            return np.empty((0, 0), dtype=np.float32)\n",
    "        vecs = np.array([d.embedding for d in data], dtype=np.float32)\n",
    "        if vecs.ndim == 1:\n",
    "            vecs = vecs.reshape(1, -1)\n",
    "\n",
    "        # Validate against sidecar / known dims if available\n",
    "        shape_path = self.vec_path + \".shape\"\n",
    "        if os.path.exists(shape_path):\n",
    "            try:\n",
    "                shp = json.loads(open(shape_path, \"r\", encoding=\"utf-8\").read())\n",
    "                d_expected = int(shp.get(\"d\"))\n",
    "                if vecs.shape[1] != d_expected:\n",
    "                    raise ValueError(f\"Embedding dimension mismatch at embed time: got {vecs.shape[1]} vs index {d_expected}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        return vecs\n",
    "\n",
    "    def add_document(self, client: OpenAI, url: str, title: Optional[str], chunks: List[str]) -> int:\n",
    "        \"\"\"\n",
    "        Add a single document (split into chunks) to the index.\n",
    "        Idempotent at chunk level via stable chunk_id hash.\n",
    "        \"\"\"\n",
    "        if not chunks:\n",
    "            return 0\n",
    "        \n",
    "        STOP = set(\"\"\"\n",
    "                the a an and or of for to in on by with without as at from into over under within between among across\n",
    "                main menu navigation contents donate help portal page pages login logout edit talk search jump sidebar\n",
    "                \"\"\".split())\n",
    "\n",
    "        records, texts, bm25_tok = [], [], []\n",
    "        for i, ch in enumerate(chunks):\n",
    "            chunk_id = _hash_id(url, str(i), ch[:64])\n",
    "            if any(m[\"chunk_id\"] == chunk_id for m in self._meta):\n",
    "                continue\n",
    "            records.append({\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"url\": url,\n",
    "                \"title\": title,\n",
    "                \"order\": i,\n",
    "                \"created_at\": dt.datetime.now(dt.timezone.utc).isoformat(),\n",
    "            })\n",
    "            texts.append(ch)\n",
    "            if USE_BM25:\n",
    "                raw = re.findall(r\"[A-Za-z0-9_]+\", ch.lower())\n",
    "                tokens = [w for w in raw if len(w) > 2 and w not in STOP]\n",
    "                bm25_tok.append({\"chunk_id\": chunk_id, \"tokens\": tokens})\n",
    "\n",
    "        if not records:\n",
    "            return 0\n",
    "\n",
    "        embeds = self.embed_texts(client, texts)\n",
    "        if embeds.ndim != 2 or embeds.shape[0] == 0:\n",
    "            return 0\n",
    "\n",
    "        docs = [{\"chunk_id\": r[\"chunk_id\"], \"text\": t} for r, t in zip(records, texts)]\n",
    "        self._append(records, embeds, docs, bm25_tok if USE_BM25 else None)\n",
    "        return len(records)\n",
    "\n",
    "    def _semantic_scores(self, query_vec: np.ndarray, top_k: int = 12) -> List[Tuple[int, float]]:\n",
    "        if self._emb is None or self._emb.shape[0] == 0:\n",
    "            return []\n",
    "        M = np.array(self._emb, dtype=np.float32)\n",
    "        q = query_vec / (np.linalg.norm(query_vec) + 1e-9)\n",
    "        M_norm = M / (np.linalg.norm(M, axis=1, keepdims=True) + 1e-9)\n",
    "        sims = (M_norm @ q.T).squeeze()\n",
    "        idx = np.argpartition(-sims, kth=min(top_k, len(sims)-1))[:top_k]\n",
    "        return sorted([(int(i), float(sims[int(i)])) for i in idx], key=lambda x: -x[1])\n",
    "\n",
    "    def _bm25_scores(self, query: str, top_k: int = 12) -> List[Tuple[int, float]]:\n",
    "        if not (USE_BM25 and self._bm25):\n",
    "            return []\n",
    "        q_tokens = re.findall(r\"[A-Za-z0-9_]+\", query.lower())\n",
    "        scores = self._bm25.get_scores(q_tokens)  # type: ignore\n",
    "        if len(scores) == 0:\n",
    "            return []\n",
    "        idx = np.argpartition(-scores, kth=min(top_k, len(scores)-1))[:top_k]\n",
    "        return sorted([(int(i), float(scores[int(i)])) for i in idx], key=lambda x: -x[1])\n",
    "\n",
    "    def search(self, client: OpenAI, query: str, k: int = 8) -> List[SearchHit]:\n",
    "        \"\"\"\n",
    "        Hybrid search (semantic + BM25 if available) with RRF + optional lexical gate,\n",
    "        MMR rerank, and query-centered snippets. Returns top-k hits.\n",
    "        \"\"\"\n",
    "    # 0) guards\n",
    "        if self._emb is None or self._emb.shape[0] == 0:\n",
    "            return []\n",
    "\n",
    "        # 1) query embedding\n",
    "        q_vec = self.embed_texts(client, [query])\n",
    "        if q_vec is None or getattr(q_vec, \"ndim\", 0) != 2 or q_vec.shape[0] == 0:\n",
    "            return []\n",
    "\n",
    "        # 2) candidates (semantic + lexical)\n",
    "        pool_size = max(k * 5, 40)\n",
    "        sem_top = self._semantic_scores(q_vec[0], top_k=pool_size)  # [(row_idx, sim)]\n",
    "        lex_top = self._bm25_scores(query, top_k=pool_size) if USE_BM25 else []\n",
    "\n",
    "        # 3) RRF fusion\n",
    "        scores: Dict[int, float] = {}\n",
    "        def fuse(items: List[Tuple[int, float]]) -> None:\n",
    "            for rank, (row_idx, _) in enumerate(items, start=1):\n",
    "                scores[row_idx] = scores.get(row_idx, 0.0) + 1.0 / (60.0 + rank)\n",
    "        fuse(sem_top)\n",
    "        fuse(lex_top)\n",
    "\n",
    "        if not scores:\n",
    "            return []\n",
    "\n",
    "        # 4) lexical gate (general): keep if any token overlap; if empty, fall back\n",
    "        q_tokens = set(_tokenize(query))\n",
    "        def _has_overlap(row_idx: int) -> bool:\n",
    "            # use precomputed tokens if available; else from text\n",
    "            cid = self._meta[row_idx][\"chunk_id\"]\n",
    "            toks = set(self._bm25_tokens.get(cid, [])) if USE_BM25 else set()\n",
    "            if not toks:\n",
    "                toks = set(_tokenize(self._docs.get(cid, \"\")))\n",
    "            return len(q_tokens & toks) > 0\n",
    "\n",
    "        filtered_scores = {i: s for i, s in scores.items() if _has_overlap(i)}\n",
    "        if not filtered_scores:\n",
    "            filtered_scores = scores  # don’t drop everything\n",
    "\n",
    "        # 5) candidate pool sorted by fused score\n",
    "        pool = sorted(filtered_scores.items(), key=lambda x: -x[1])[:max(k * 3, 24)]\n",
    "        cand_idx = [i for i, _ in pool]\n",
    "\n",
    "        # 6) MMR rerank (fallback to fused order if anything goes wrong)\n",
    "        try:\n",
    "            M = np.array(self._emb, dtype=np.float32)\n",
    "            mmr_ordered = _mmr_rerank(cand_idx, q_vec[0], M, k=k)  # returns list[int]\n",
    "        except Exception:\n",
    "            mmr_ordered = cand_idx[:k]\n",
    "\n",
    "        # 7) Build hits with query-centered snippets\n",
    "        hits: List[SearchHit] = []\n",
    "        for row_idx in mmr_ordered[:k]:\n",
    "            if row_idx < 0 or row_idx >= len(self._meta):\n",
    "                continue\n",
    "            meta = self._meta[row_idx]\n",
    "            chunk_id = meta[\"chunk_id\"]\n",
    "            text = self._docs.get(chunk_id, \"\")\n",
    "            snippet = _build_snippet(text, query)\n",
    "            # score = fused score for transparency (not strictly needed for ranking now)\n",
    "            fused_score = float(filtered_scores.get(row_idx, 0.0))\n",
    "            hits.append(\n",
    "                SearchHit(\n",
    "                    url=meta[\"url\"],\n",
    "                    title=meta.get(\"title\"),\n",
    "                    chunk_id=chunk_id,\n",
    "                    score=fused_score,\n",
    "                    snippet=snippet,\n",
    "                )\n",
    "            )\n",
    "        return hits\n",
    "\n",
    "\n",
    "\n",
    "# ---------------- Tools: fetch_content, save_summary, index_links, search_links and answer_question ----------------\n",
    "\n",
    "_SUMMARY_KEYS_SEEN: set[tuple[str, str, str]] = set()  # (url, style, saved_path)\n",
    "\n",
    "def fetch_content(url: HttpUrl) -> FetchOutput:\n",
    "    \"\"\"Fetch full text from a webpage.\"\"\"\n",
    "    text, title = _fetch_text_and_title(str(url))\n",
    "    return FetchOutput(url=url, title=title, text=text)\n",
    "\n",
    "def save_summary(\n",
    "    url: HttpUrl,\n",
    "    style: Literal[\"bullet\", \"exec\"] = \"bullet\",\n",
    "    out_dir: str = \"summaries\",\n",
    "    model_name: Optional[str] = None,\n",
    ") -> SummaryOutput:\n",
    "    \"\"\"\n",
    "    One-shot: fetch full text from URL, summarize with OpenAI, save to a single JSON file\n",
    "    named {slug}_summary.json under `out_dir`, and return the saved path.\n",
    "    \"\"\"\n",
    "    # Fetch content\n",
    "    text, title = _fetch_text_and_title(str(url))\n",
    "\n",
    "    # Compose filename\n",
    "    slug = _slug_from_url(str(url))\n",
    "    os.makedirs(out_dir or \".\", exist_ok=True)\n",
    "    saved_path = os.path.join(out_dir, f\"{slug}_summary.json\")\n",
    "\n",
    "    key = (str(url), style, saved_path)\n",
    "    if key in _SUMMARY_KEYS_SEEN:\n",
    "        return SummaryOutput(url=url, title=title, summary=\"(already summarized in this run)\", saved_to=saved_path)\n",
    "\n",
    "    # OpenAI client\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"OPENAI_API_KEY must be set\")\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    model = model_name or os.getenv(\"AGENT_MODEL\", \"gpt-4o-mini\")\n",
    "\n",
    "    style_instr = (\n",
    "        \"Return 5–8 concise bullets with key facts, numbers, and dates. End with a one-sentence takeaway.\"\n",
    "        if style == \"bullet\"\n",
    "        else \"Write a tight 2–3 paragraph executive summary followed by 3 concrete action items.\"\n",
    "    )\n",
    "\n",
    "    max_chars = 120_000\n",
    "    body = text if len(text) <= max_chars else (text[:max_chars] + \"\\n...[truncated]\")\n",
    "\n",
    "    prompt = f\"Title: {title or 'N/A'}\\nURL: {url}\\n\\n{style_instr}\\n\\nCONTENT:\\n{body}\"\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.2,\n",
    "        max_tokens=500,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a precise summarizer. Stay faithful to the source.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    summary = (resp.choices[0].message.content or \"\").strip()\n",
    "\n",
    "    record = {\n",
    "        \"url\": str(url),\n",
    "        \"title\": title,\n",
    "        \"summary\": summary,\n",
    "        \"model\": model,\n",
    "        \"created_at\": dt.datetime.now(dt.timezone.utc).isoformat(),\n",
    "        \"saved_to\": saved_path,\n",
    "    }\n",
    "    with open(saved_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(record, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    _SUMMARY_KEYS_SEEN.add(key)\n",
    "    return SummaryOutput(url=url, title=title, summary=summary, saved_to=saved_path)\n",
    "\n",
    "\n",
    "def index_links(params: IndexLinksParams) -> IndexStats:\n",
    "    return _index_links_impl(**params.model_dump())\n",
    "\n",
    "def search_links(\n",
    "    query: str,\n",
    "    index_dir: str = \"index\",\n",
    "    embed_model: str = \"text-embedding-ada-002\",\n",
    "    k: int = 8,\n",
    ") -> List[SearchHit]:\n",
    "    \"\"\"\n",
    "    Search the link index and return top-k chunk hits with snippets and scores.\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"OPENAI_API_KEY must be set\")\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    idx = VectorIndex(index_dir=index_dir, embedding_model=embed_model)\n",
    "    return idx.search(client, query, k=k)\n",
    "\n",
    "def answer_question(\n",
    "    question: str,\n",
    "    index_dir: str = \"index\",\n",
    "    gen_model: str = None,\n",
    "    embed_model: str = \"text-embedding-ada-002\",\n",
    "    k: int = 8,\n",
    "    max_context_chars: int = 18_000,\n",
    ") -> AnswerOutput:\n",
    "    \"\"\"\n",
    "    RAG-style answer grounded on the indexed links, with lightweight citations.\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"OPENAI_API_KEY must be set\")\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    gen_model = gen_model or os.getenv(\"AGENT_MODEL\", \"gpt-4o-mini\")\n",
    "\n",
    "    idx = VectorIndex(index_dir=index_dir, embedding_model=embed_model)\n",
    "    hits = idx.search(client, question, k=k)\n",
    "    if not hits:\n",
    "        return AnswerOutput(\n",
    "            question=question,\n",
    "            answer=\"I couldn't find anything relevant in the current link index.\",\n",
    "            citations=[],\n",
    "            used_model=gen_model,\n",
    "        )\n",
    "\n",
    "    # Build context (trim to max_context_chars)\n",
    "    ctx_blocks = []\n",
    "    citations = []\n",
    "    used_chars = 0\n",
    "    for i, h in enumerate(hits, start=1):\n",
    "        block = f\"[{i}] Title: {h.title or 'N/A'}\\nURL: {h.url}\\nCHUNK_ID: {h.chunk_id}\\nSNIPPET:\\n{h.snippet}\\n\"\n",
    "        if used_chars + len(block) > max_context_chars:\n",
    "            break\n",
    "        ctx_blocks.append(block)\n",
    "        citations.append({\"url\": str(h.url), \"title\": h.title or \"N/A\", \"chunk_id\": h.chunk_id})\n",
    "        used_chars += len(block)\n",
    "\n",
    "    system = (\n",
    "        \"You are a careful research assistant. Answer ONLY using the provided context. \"\n",
    "        \"If the context is insufficient, say so. Include bracketed citation numbers [1], [2] inline \"\n",
    "        \"corresponding to the provided context blocks.\"\n",
    "    )\n",
    "    user = f\"QUESTION:\\n{question}\\n\\nCONTEXT:\\n\" + \"\\n---\\n\".join(ctx_blocks)\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=gen_model,\n",
    "        temperature=0.2,\n",
    "        max_tokens=600,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": user},\n",
    "        ],\n",
    "    )\n",
    "    answer = (resp.choices[0].message.content or \"\").strip()\n",
    "    return AnswerOutput(\n",
    "        question=question,\n",
    "        answer=answer,\n",
    "        citations=citations,\n",
    "        used_model=gen_model,\n",
    "    )\n",
    "\n",
    "def answer_question_debug(\n",
    "    question: str,\n",
    "    index_dir: str = \"index\",\n",
    "    gen_model: str = None,\n",
    "    embed_model: str = \"text-embedding-ada-002\",\n",
    "    k: int = 8,\n",
    "    max_context_chars: int = 18_000,\n",
    "):\n",
    "    \"\"\"Same as answer_question but also returns the retrieved hits.\"\"\"\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"OPENAI_API_KEY must be set\")\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    gen_model = gen_model or os.getenv(\"AGENT_MODEL\", \"gpt-4o-mini\")\n",
    "\n",
    "    idx = VectorIndex(index_dir=index_dir, embedding_model=embed_model)\n",
    "    hits = idx.search(client, question, k=k)\n",
    "\n",
    "    if not hits:\n",
    "        return AnswerOutput(\n",
    "            question=question,\n",
    "            answer=\"I couldn't find anything relevant in the current link index.\",\n",
    "            citations=[],\n",
    "            used_model=gen_model,\n",
    "        ), []\n",
    "\n",
    "    # same logic as before\n",
    "    ctx_blocks, citations, used_chars = [], [], 0\n",
    "    for i, h in enumerate(hits, start=1):\n",
    "        block = f\"[{i}] Title: {h.title or 'N/A'}\\nURL: {h.url}\\nCHUNK_ID: {h.chunk_id}\\nSNIPPET:\\n{h.snippet}\\n\"\n",
    "        if used_chars + len(block) > max_context_chars:\n",
    "            break\n",
    "        ctx_blocks.append(block)\n",
    "        citations.append({\"url\": str(h.url), \"title\": h.title or \"N/A\", \"chunk_id\": h.chunk_id})\n",
    "        used_chars += len(block)\n",
    "\n",
    "    system = (\n",
    "        \"You are a careful research assistant. Answer ONLY using the provided context. \"\n",
    "        \"If the context is insufficient, say so. Include bracketed citation numbers [1], [2] inline \"\n",
    "        \"corresponding to the provided context blocks.\"\n",
    "    )\n",
    "    user = f\"QUESTION:\\n{question}\\n\\nCONTEXT:\\n\" + \"\\n---\\n\".join(ctx_blocks)\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=gen_model,\n",
    "        temperature=0.2,\n",
    "        max_tokens=600,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": user},\n",
    "        ],\n",
    "    )\n",
    "    answer = (resp.choices[0].message.content or \"\").strip()\n",
    "    return AnswerOutput(\n",
    "        question=question,\n",
    "        answer=answer,\n",
    "        citations=citations,\n",
    "        used_model=gen_model,\n",
    "    ), hits\n",
    "\n",
    "# ---------------- Agent wiring ----------------\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You have tools to (a) fetch/summarize a single URL and (b) build/search an index of links and answer questions.\n",
    "\n",
    "Tools:\n",
    "- fetch_content(url)\n",
    "- save_summary(url, style?, out_dir?, model_name?)\n",
    "- index_links(params)  # params: {urls, index_dir, embed_model, chunk_tokens, overlap_tokens, connect_timeout, read_timeout}\n",
    "- search_links(query, index_dir?, embed_model?, k?)\n",
    "- answer_question_debug(question, index_dir?, gen_model?, embed_model?, k?, max_context_chars?)\n",
    "\n",
    "Behavioral rules:\n",
    "• If the user asks to fetch AND save a summary in one request, call ONLY save_summary ONCE.\n",
    "• Never call save_summary more than once per user request.\n",
    "• For questions about already-indexed links, call answer_question.\n",
    "• If the user provides new links to add, call index_links first, then answer_question if they asked a question.\n",
    "• Always surface citations (URLs) in answers that rely on the index.\n",
    "• Be precise and avoid hallucinations; if the context lacks an answer, say so.\n",
    "\"\"\".strip()\n",
    "\n",
    "agent = Agent(\n",
    "    model=os.getenv(\"AGENT_MODEL\", \"gpt-4o-mini\"),\n",
    "    tools=[fetch_content, save_summary, index_links, search_links, answer_question_debug],\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c69d1332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added_docs=41 total_docs=41 index_dir='index' embedding_model='text-embedding-ada-002' bm25_enabled=True updated_at=datetime.datetime(2025, 11, 1, 22, 31, 20, 464084, tzinfo=datetime.timezone.utc)\n"
     ]
    }
   ],
   "source": [
    "import shutil; shutil.rmtree(\"index\", ignore_errors=True)\n",
    "\n",
    "stats = index_links(IndexLinksParams(\n",
    "    urls=[\n",
    "        \"https://en.wikipedia.org/wiki/Capybara\",\n",
    "        \"https://en.wikipedia.org/wiki/Lesser_capybara\",\n",
    "        \"https://en.wikipedia.org/wiki/Hydrochoerus\",\n",
    "        \"https://en.wikipedia.org/wiki/Neochoerus\",\n",
    "        \"https://en.wikipedia.org/wiki/Caviodon\",\n",
    "        \"https://en.wikipedia.org/wiki/Neochoerus_aesopi\",\n",
    "    ],\n",
    "    index_dir=\"index\",\n",
    "    embed_model=\"text-embedding-ada-002\",  # keep consistent\n",
    "))\n",
    "print(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5e7ee103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ANSWER ===\n",
      "The threats to capybara populations include hunting, which has reduced their numbers in some areas. However, overall, capybaras are not considered a threatened species, and their population is stable throughout most of their South American range [1].\n",
      "\n",
      "=== TOP HITS ===\n",
      "0.033 | Capybara | https://en.wikipedia.org/wiki/Capybara\n",
      "Conservation and human interaction Capybaras are not considered a threatened species; [ 1 ] their population is stable throughout most of their South American range, though in some areas hunting has reduced their numbers……\n",
      "---\n",
      "0.029 | Lesser capybara | https://en.wikipedia.org/wiki/Lesser_capybara\n",
      "…/IUCN.UK.2016-2.RLTS.T136277A22189896.en . Retrieved 19 November 2021 . [2] canopytower (2021-03-12). \"Lesser Capybara | The Canopy Family\" . Retrieved 2024-10-21 . [3] Schmidt, Amanda (2023-08-14). \"Capybara Fact Sheet ……\n",
      "---\n",
      "0.027 | Hydrochoerus | https://en.wikipedia.org/wiki/Hydrochoerus\n",
      "Behavior Capybaras are highly social, living in groups of up to 100 and communicating through a variety of vocalizations . [ 2 ] Breeding is polygynous , with males forming harems .…\n",
      "---\n",
      "0.027 | Lesser capybara | https://en.wikipedia.org/wiki/Lesser_capybara\n",
      "Adaptations As animals that are considered prey for many animals, the species is very wary of predators and likes to travel in groups of about 20 cavies. [ 5 ] As a result, the capybaras have adapted to be excellent swim……\n",
      "---\n",
      "0.029 | Lesser capybara | https://en.wikipedia.org/wiki/Lesser_capybara\n",
      "Habitat The lesser capybara mainly inhabits areas close to water such as marshes, ponds, and lagoon habitats as these places offer water, which is essential for these capybaras to fulfil their niches' of maintain body te……\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "question = \"What are threats to capybara populations?\"\n",
    "ans, hits = answer_question_debug(question,\n",
    "                                  index_dir=\"index\",\n",
    "                                  embed_model=\"text-embedding-ada-002\",\n",
    "                                  k=5)\n",
    "\n",
    "print(\"\\n=== ANSWER ===\")\n",
    "print(ans.answer)\n",
    "print(\"\\n=== TOP HITS ===\")\n",
    "for h in hits:\n",
    "    print(round(h.score, 3), \"|\", h.title, \"|\", h.url)\n",
    "    print(h.snippet[:300] + \"…\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ab1437d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456764e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added_docs=24 total_docs=24 index_dir='index_ada' embedding_model='text-embedding-ada-002' bm25_enabled=True updated_at=datetime.datetime(2025, 11, 1, 20, 59, 14, 299471, tzinfo=datetime.timezone.utc)\n"
     ]
    }
   ],
   "source": [
    "URLS = [\n",
    "  \"https://en.wikipedia.org/wiki/Capybara\",\n",
    "  \"https://en.wikipedia.org/wiki/Lesser_capybara\",\n",
    "  \"https://en.wikipedia.org/wiki/Hydrochoerus\",\n",
    "  \"https://en.wikipedia.org/wiki/Neochoerus\",\n",
    "  \"https://en.wikipedia.org/wiki/Caviodon\",\n",
    "  \"https://en.wikipedia.org/wiki/Neochoerus_aesopi\",\n",
    "]\n",
    "EMBED = \"text-embedding-ada-002\"\n",
    "NEW_DIR = \"index_ada\"\n",
    "\n",
    "stats = index_links(IndexLinksParams(urls=URLS, index_dir=NEW_DIR, embed_model=EMBED))\n",
    "print(stats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "031d6818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANSWER ===\n",
      "I couldn't find anything relevant in the current link index.\n",
      "\n",
      "=== SEARCH HITS ===\n"
     ]
    }
   ],
   "source": [
    "q = \"What are threats to capybara populations?\"\n",
    "ans, hits = answer_question_debug(q, index_dir=\"index\", embed_model=\"text-embedding-ada-002\", k=5)\n",
    "\n",
    "print(\"=== ANSWER ===\")\n",
    "print(ans.answer)\n",
    "print(\"\\n=== SEARCH HITS ===\")\n",
    "for h in hits:\n",
    "    print(f\"{round(h.score, 3)} | {h.title} | {h.url}\")\n",
    "    print(h.snippet[:300] + \"…\")\n",
    "    print(\"---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
